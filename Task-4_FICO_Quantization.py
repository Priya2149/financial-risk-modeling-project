# -*- coding: utf-8 -*-
"""JPMC_Task_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1en4rQHfD2VMo8ShZGNeiR4KpwggNsuIf
"""

# Task 4 – FICO Score Bucketing
# ---------------------------------------------
# In this task we group borrower FICO scores into a set number of buckets (ratings).
# Lower rating numbers mean better credit quality.
#
# The code supports two ways of creating the buckets:
#   1. Mean Squared Error (MSE): tries to keep FICO scores in each bucket close together.
#   2. Log-Likelihood (LL): looks at how many defaults happen in each bucket and finds splits
#     that best explain the pattern of defaults.
#
# For each bucket we show: the FICO score range, how many borrowers are in it,
# how many defaults occurred, and the default rate (PD).
# The output also includes a helper function so you can map any borrower’s FICO score to the right rating bucket.

import numpy as np
import pandas as pd
from typing import Literal, Dict, Any, Tuple, List

def _prefix_sums(x: np.ndarray, y: np.ndarray) -> Dict[str, np.ndarray]:
    """Build prefix sums for fast segment stats.
       x: fico scores (float), y: default indicator (0/1)"""
    n = len(x)
    one = np.ones(n, dtype=float)
    ps = {
        "cnt": np.concatenate([[0.0], np.cumsum(one)]),
        "sumx": np.concatenate([[0.0], np.cumsum(x)]),
        "sumxx": np.concatenate([[0.0], np.cumsum(x * x)]),
        "sumy": np.concatenate([[0.0], np.cumsum(y)]),
    }
    return ps

def _seg_counts(ps: Dict[str, np.ndarray], i: int, j: int) -> Tuple[float, float, float]:
    """Stats on x[i:j] (j exclusive). Returns (n, sumx, sumxx)."""
    n = ps["cnt"][j] - ps["cnt"][i]
    sx = ps["sumx"][j] - ps["sumx"][i]
    sxx = ps["sumxx"][j] - ps["sumxx"][i]
    return n, sx, sxx

def _seg_defaults(ps: Dict[str, np.ndarray], i: int, j: int) -> Tuple[float, float]:
    """Defaults on y[i:j] (j exclusive). Returns (n, k)."""
    n = ps["cnt"][j] - ps["cnt"][i]
    k = ps["sumy"][j] - ps["sumy"][i]
    return n, k

# Segment objectives

def _mse_cost(ps: Dict[str, np.ndarray], i: int, j: int) -> float:
    """Within-segment SSE (sum of squared errors) for x[i:j]."""
    n, sx, sxx = _seg_counts(ps, i, j)
    if n <= 0:
        return 0.0

    return float(sxx - (sx * sx) / n)

def _ll_score(ps: Dict[str, np.ndarray], i: int, j: int, alpha: float = 1e-6) -> float:
    """Binomial log-likelihood for segment y[i:j] with Laplace/Jeffreys smoothing."""
    n, k = _seg_defaults(ps, i, j)
    if n <= 0:
        return 0.0

    p = (k + alpha) / (n + 2 * alpha)
    return float(k * np.log(p) + (n - k) * np.log(1 - p))


# Dynamic programming

def _optimal_buckets(
    x_sorted: np.ndarray,
    y_sorted: np.ndarray,
    K: int,
    objective: Literal["mse", "ll"] = "ll",
    smoothing: float = 1e-6,
) -> Tuple[List[int], float]:
    """
    Return cut indices (monotone, ending at N) that maximize (LL) or minimize (MSE).
    For MSE we minimize total SSE; for LL we maximize total LL.
    """
    N = len(x_sorted)
    ps = _prefix_sums(x_sorted, y_sorted)
    # dp[k][j]: best objective using first j points into k buckets.
    # back[k][j]:k-1 buckets on first i, last bucket i:j
    if objective == "mse":
        # we minimize cost
        dp = np.full((K + 1, N + 1), np.inf)
        back = np.full((K + 1, N + 1), -1, dtype=int)
        dp[0, 0] = 0.0
        for k in range(1, K + 1):
            for j in range(1, N + 1):
                # try all i < j
                best = np.inf
                best_i = -1
                for i in range(k - 1, j):  # at least one per bucket
                    c = dp[k - 1, i] + _mse_cost(ps, i, j)
                    if c < best:
                        best, best_i = c, i
                dp[k, j], back[k, j] = best, best_i
        # recover cuts
        cuts = [N]
        j = N
        for k in range(K, 0, -1):
            i = back[k, j]
            cuts.append(i)
            j = i
        cuts = sorted(cuts)  # indices 0=i0 < i1 < ... < iK=N
        return cuts, float(dp[K, N])
    else:
        # maximize score
        dp = np.full((K + 1, N + 1), -np.inf)
        back = np.full((K + 1, N + 1), -1, dtype=int)
        dp[0, 0] = 0.0
        for k in range(1, K + 1):
            for j in range(1, N + 1):
                best = -np.inf
                best_i = -1
                for i in range(k - 1, j):
                    s = dp[k - 1, i] + _ll_score(ps, i, j, alpha=smoothing)
                    if s > best:
                        best, best_i = s, i
                dp[k, j], back[k, j] = best, best_i
        cuts = [N]
        j = N
        for k in range(K, 0, -1):
            i = back[k, j]
            cuts.append(i)
            j = i
        cuts = sorted(cuts)
        return cuts, float(dp[K, N])

def fit_fico_rating_map(
    df: pd.DataFrame,
    fico_col: str = "fico_score",
    target_col: str = "default",
    n_buckets: int = 10,
    objective: Literal["mse", "ll"] = "ll",
    smoothing: float = 1e-6,
) -> Dict[str, Any]:
    """
    Fit an optimal quantization of FICO into n_buckets.
    Returns bucket boundaries and a mapping function rating(fico) with 1=best, n_buckets=worst.
    """
    # keep only needed cols, drop NA, sort by FICO ascending
    data = df[[fico_col, target_col]].dropna().copy()
    data = data.sort_values(fico_col, kind="mergesort").reset_index(drop=True)
    x = data[fico_col].to_numpy(dtype=float)
    y = data[target_col].to_numpy(dtype=float)

    # dynamic programming over rows
    cuts, obj = _optimal_buckets(x, y, K=n_buckets, objective=objective, smoothing=smoothing)

    # turn row-index cuts into numeric boundaries

    boundaries = []
    for i in range(1, len(cuts) - 0):
        if i < len(cuts) - 1:
            idx_end = cuts[i+1] - 1
            boundaries.append(float(x[idx_end]))
    # boundaries list length is n_buckets
    if len(boundaries) < n_buckets:
        boundaries.append(float(x[-1]))

    # Build summary per bucket
    rows = []
    start = cuts[0]
    for b in range(1, len(cuts)):
        i0, i1 = cuts[b-1], cuts[b]
        seg = data.iloc[i0:i1]
        n = len(seg)
        k = float(seg[target_col].sum())
        p = k / n if n > 0 else np.nan
        lo = float(seg[fico_col].min()) if n else np.nan
        hi = float(seg[fico_col].max()) if n else np.nan
        rows.append({"bucket_idx": b-1, "fico_min": lo, "fico_max": hi, "n": n, "k": int(k), "pd_hat": p})
    summary = pd.DataFrame(rows)

    # Map: higher FICO → better (lower) rating number.

    # Rating = n_buckets - bucket_idx
    summary["rating"] = n_buckets - summary["bucket_idx"]

    bounds = summary[["fico_max", "rating"]].sort_values("fico_max").to_numpy()

    def map_fico_to_rating(fico: float) -> int:
        for bmax, r in bounds:
            if fico <= bmax:
                return int(r)
        return int(summary["rating"].iloc[0])

    return {
        "objective": objective,
        "score": obj,
        "boundaries": [float(b) for b in summary["fico_max"]],
        "table": summary.sort_values("rating"),
        "map_func": map_fico_to_rating,
    }

# Example usage

df = pd.read_csv("Task 3 and 4_Loan_Data.csv")
result_ll = fit_fico_rating_map(df, fico_col="fico_score", target_col="default",
                                n_buckets=10, objective="ll", smoothing=1e-6)
print("Log-likelihood objective:", result_ll["score"])
print(result_ll["table"])
print("Boundaries (max FICO per bucket):", result_ll["boundaries"])
print("Sample rating for FICO=720:", result_ll)